{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e2a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad602ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('IMDB Dataset.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de60bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>Once again, I've been duped by seemingly intel...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27544</th>\n",
       "      <td>Horror movie??really???? i cant believe how ba...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36309</th>\n",
       "      <td>Typical 90's comedy, situational comedy simila...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>To quote Clark Griswold (in the original Chris...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>The Theory Of Flight is an engaging character ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "3050   Once again, I've been duped by seemingly intel...  negative\n",
       "27544  Horror movie??really???? i cant believe how ba...  negative\n",
       "36309  Typical 90's comedy, situational comedy simila...  positive\n",
       "2958   To quote Clark Griswold (in the original Chris...  negative\n",
       "11432  The Theory Of Flight is an engaging character ...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9aad3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d738d7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lower case\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c238df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f34897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b68bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove HTML tags\n",
    "import re\n",
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2a7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\"\n",
    "#text=df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c55e18cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83898029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285c2e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42158</th>\n",
       "      <td>this is not as funny and gory as the dvd box c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437</th>\n",
       "      <td>i am fully aware there is no statistical data ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "42158  this is not as funny and gory as the dvd box c...  negative\n",
       "5437   i am fully aware there is no statistical data ...  negative"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6afe6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing urls\n",
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850e7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1='check out my notebook https://www.kaggle.com/code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b3b2174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check out my notebook '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c836152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548a2e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove punctuation\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a18d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9359c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda2f324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string with  punctuation'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2='string& with \\\\ punctuation'\n",
    "remove_punc(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eadce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bb5ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat word treatment\n",
    "abbreviations=[(\"AFAIK\",\"As Far I Know\"),\n",
    "              (\"AFK\",\"Away From Keyword\"),\n",
    "              (\"ASAP\",\"As Soon As Possible\"),\n",
    "              (\"ATK\",\"At The Keyword\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dadb1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFAIK': 'As Far I Know', 'AFK': 'Away From Keyword', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyword'}\n"
     ]
    }
   ],
   "source": [
    "abbreviations_dict={abbr:meaning for abbr,meaning in abbreviations}\n",
    "print(abbreviations_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38577ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper()in abbreviations_dict:\n",
    "            new_text.append(abbreviations_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582d5d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At The Keyword'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion(\"ATK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a1cc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a7e4acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahulpathak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbc7fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f68a5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a253950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54ada6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_text.append(word)\n",
    "            \n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4422dd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probaly all-time favorite movie,a story selflessness'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probaly my all-time favorite movie,a story of selflessness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40e55113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello indian'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('hello i am indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9665e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\rahulpathak\\appdata\\roaming\\python\\python311\\site-packages (2.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n"
     ]
    }
   ],
   "source": [
    "#removing emoji wit text\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b4de741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00f15748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love python :smiling_face_with_smiling_eyes:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('I love python ðŸ˜Š'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a5633fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "059c8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600'-\\U0001F64F\"\n",
    "                               \"\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\"\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d29e2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I love python\"\n",
    "#remove_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c1d4e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b34727f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'am', 'going', 'delhi']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization using split function\n",
    "sent1='T am going delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01ef1d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" let's hope the trip to be great\",\n",
       " '']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "sent2=\"I am going to delhi. I will stay there for 3 days. let's hope the trip to be great.\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "440aa8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'delhi!.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#problem with split\n",
    "sent3='I am going delhi!.'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e17fbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go', ' I have 3 day holiday']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4='Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7c72815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f614e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e972bab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'delhi', '!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1='I am going delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "443103ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\" Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's stsndard dummy text ever since the 1500s,.\n",
    "when an unknown printer took a gallery of type  and scrembled it to make a type specimen took\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb794b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's stsndard dummy text ever since the 1500s,.\",\n",
       " 'when an unknown printer took a gallery of type  and scrembled it to make a type specimen took']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3cbc8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '?',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'industry',\n",
       " \"'s\",\n",
       " 'stsndard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'the',\n",
       " '1500s',\n",
       " ',',\n",
       " '.',\n",
       " 'when',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'a',\n",
       " 'gallery',\n",
       " 'of',\n",
       " 'type',\n",
       " 'and',\n",
       " 'scrembled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'took']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e17ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5='I have a Ph.D in A.I.'\n",
    "sent6=\"we're here to help! mail us at abc@gamail.com\"\n",
    "sent7=\"A 5km ride cost $10.50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68ef2c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I', '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ae37c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'abc',\n",
       " '@',\n",
       " 'gamail.com']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9aa1dc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0518f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5f05898",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0db96b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample='walk walks walking walked'\n",
    "stem_word(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80e25442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"probably my alltime favorite movie a story of selflessness sacrifice and dedication\"\n",
    "stem_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2db03fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahulpathak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6ebf21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "the                 the                 \n",
      "same                same                \n",
      "time.He             time.He             \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "wl=WordNetLemmatizer()\n",
    "sentence =\"He was running and eating at the same time.He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words=nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "        \n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wl.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fc30ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text represntation?text vectorization?feature extraction\n",
    "#text - number\n",
    "#core idea - To retain its sematic meaning\n",
    "#corpus - total words in text\n",
    "#vocabulary - no. of unique words\n",
    "# document -   review           sentiment\n",
    "            #1{document}           1\n",
    "#word -each word of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41d6470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#people watch campus\n",
    "#campus watch campus\n",
    "#people write comment\n",
    "#campus write comment\n",
    "#onehotencoding\n",
    "#disadvantage\n",
    "#1.sparse matrix --> overfitting\n",
    "#2.dimension vary --> no fixed size\n",
    "#3.out of vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398efc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG OF WORDS\n",
    "#count the number of a words use in a sentence\n",
    "#sparsity\n",
    "#fixed dimension\n",
    "#out of vocabulary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
